#!/usr/bin/env python
"""Action-only baseline classifier for reason clusters (SBERT only)."""

import argparse
import json
from pathlib import Path

import numpy as np
from sklearn.linear_model import LogisticRegression, SGDClassifier
from sklearn.metrics import accuracy_score, f1_score
from sklearn.preprocessing import LabelEncoder
from sklearn.utils import shuffle as sklearn_shuffle

try:
    from sentence_transformers import SentenceTransformer
except ImportError:
    SentenceTransformer = None

try:
    from tqdm.auto import tqdm
except ImportError:
    tqdm = None

TRAIN_SIZE = 21143
VAL_SIZE = 2519


def load_dataset(path: Path):
    with path.open("r", encoding="utf-8") as f:
        data = json.load(f)

    for item in data:
        gidx = item.get("global_index", item.get("index", 0))
        split = item.get("split")
        if split in {"training", "validation", "testing"}:
            continue
        if gidx < TRAIN_SIZE:
            item["split"] = "training"
        elif gidx < TRAIN_SIZE + VAL_SIZE:
            item["split"] = "validation"
        else:
            item["split"] = "testing"
    return data


def encode_texts_sbert(texts, model_name=None):
    if SentenceTransformer is None:
        raise ImportError("sentence-transformers is not installed, cannot use S-BERT encoding")
    model_name = model_name or "all-MiniLM-L6-v2"
    model = SentenceTransformer(model_name)
    return model.encode(texts, show_progress_bar=True, convert_to_numpy=True)


def topk_accuracy(probs, labels, k=3):
    topk = np.argsort(probs, axis=1)[:, ::-1][:, :k]
    correct = sum(int(true in topk_row) for true, topk_row in zip(labels, topk))
    return correct / len(labels)


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument(
        "--data",
        type=Path,
        default=Path("driving-explanation-retrieval/data_analysis/clustering_results/reasons_with_clusters.json"),
        help="Path to clustered reasons JSON"
    )
    parser.add_argument(
        "--model-name",
        type=str,
        default="all-MiniLM-L6-v2",
        help="SBERT model name"
    )
    parser.add_argument(
        "--output",
        type=Path,
        default=Path("driving-explanation-retrieval/result/baseline/action_only_results.json"),
        help="Where to save metrics JSON"
    )
    parser.add_argument(
        "--features",
        type=Path,
        default=None,
        help="Optional feature cache (.npz) generated by prepare_action_features.py"
    )
    parser.add_argument(
        "--max-iter",
        type=int,
        default=2000,
        help="Maximum solver iterations for LogisticRegression trainer"
    )
    parser.add_argument(
        "--sgd",
        action="store_true",
        help="Use SGDClassifier with per-epoch tqdm progress instead of LogisticRegression"
    )
    parser.add_argument(
        "--epochs",
        type=int,
        default=10,
        help="Epochs for SGDClassifier training (when --sgd is set)"
    )
    parser.add_argument(
        "--log-every",
        type=int,
        default=1,
        help="Log metrics every N epochs during SGD training"
    )
    parser.add_argument(
        "--alpha",
        type=float,
        default=0.0001,
        help="Regularization strength (alpha) for SGDClassifier (ignored without --sgd)"
    )
    parser.add_argument(
        "--learning-rate",
        choices=["optimal", "constant", "invscaling", "adaptive"],
        default="optimal",
        dest="learning_rate",
        help="Learning rate schedule for SGDClassifier (ignored without --sgd)"
    )
    parser.add_argument(
        "--eta0",
        type=float,
        default=0.0,
        help="Initial learning rate for SGDClassifier when applicable"
    )
    parser.add_argument(
        "--random-state",
        type=int,
        default=42,
        help="Random seed for shuffling (SGD mode) and model initialization"
    )
    parser.add_argument(
        "--no-tqdm",
        action="store_true",
        help="Disable tqdm progress bars even when --sgd is enabled"
    )
    parser.add_argument(
        "--verbose",
        action="store_true",
        help="Print solver diagnostics (LogisticRegression only)"
    )
    args = parser.parse_args()

    if args.features:
        cache = np.load(args.features, allow_pickle=True)
        X_train = cache["X_train"]
        X_val = cache["X_val"]
        X_test = cache["X_test"]
        y_train = cache["y_train"]
        y_val = cache["y_val"]
        y_test = cache["y_test"]
        encoder_used = str(cache.get("encoder", "sbert"))
        model_name = str(cache.get("model", "all-MiniLM-L6-v2"))
        label_encoder = LabelEncoder()
        label_encoder.classes_ = cache["label_classes"]
    else:
        data = load_dataset(args.data)
        actions = [item["action"] for item in data]
        labels = [item["cluster"] for item in data]
        splits = np.array([item["split"] for item in data])

        mask_train = splits == "training"
        mask_val = splits == "validation"
        mask_test = splits == "testing"

        model_name = args.model_name or "all-MiniLM-L6-v2"
        print(f"Encoding actions with SBERT ({model_name})...")
        embeddings = encode_texts_sbert(actions, model_name=model_name)
        X_train = embeddings[mask_train]
        X_val = embeddings[mask_val]
        X_test = embeddings[mask_test]

        label_encoder = LabelEncoder()
        y = label_encoder.fit_transform(labels)
        y_train = y[mask_train]
        y_val = y[mask_val]
        y_test = y[mask_test]

    def evaluate(X, y_true, name):
        probs = clf.predict_proba(X)
        top1_pred = probs.argmax(axis=1)
        top1 = accuracy_score(y_true, top1_pred)
        top3 = topk_accuracy(probs, y_true, k=3)
        macro_f1 = f1_score(y_true, top1_pred, average="macro")
        print(f"{name}: Top-1={top1:.4f} | Top-3={top3:.4f} | Macro-F1={macro_f1:.4f}")
        return {"top1": top1, "top3": top3, "macro_f1": macro_f1}

    if args.sgd:
        if tqdm is None and not args.no_tqdm:
            raise ImportError("tqdm is not installed, cannot display training progress. Please install tqdm or use --no-tqdm")
        classes = np.unique(y_train)
        clf = SGDClassifier(
            loss="log_loss",
            penalty="l2",
            alpha=args.alpha,
            learning_rate=args.learning_rate,
            eta0=args.eta0,
            max_iter=1,
            tol=None,
            random_state=args.random_state
        )
        epoch_iterable = range(args.epochs)
        if tqdm is not None and not args.no_tqdm:
            epoch_iterable = tqdm(epoch_iterable, desc="SGD epochs", unit="epoch")
        for epoch in epoch_iterable:
            X_epoch, y_epoch = sklearn_shuffle(
                X_train,
                y_train,
                random_state=args.random_state + epoch
            )
            clf.partial_fit(X_epoch, y_epoch, classes=classes)
            if (epoch + 1) % max(1, args.log_every) == 0:
                train_metrics = evaluate(X_train, y_train, f"Train@{epoch + 1}")
                val_metrics = evaluate(X_val, y_val, f"Val@{epoch + 1}")
                if isinstance(epoch_iterable, list):
                    print(
                        f"Epoch {epoch + 1}/{args.epochs} "
                        f"- Train Top-1 {train_metrics['top1']:.4f}, "
                        f"Val Top-1 {val_metrics['top1']:.4f}"
                    )
        trainer_used = "sgd"
    else:
        solver = "lbfgs"
        logreg_kwargs = {
            "max_iter": args.max_iter,
            "solver": solver,
            "random_state": args.random_state
        }
        if args.verbose:
            logreg_kwargs["verbose"] = 1
        clf = LogisticRegression(**logreg_kwargs)
        clf.fit(X_train, y_train)
        trainer_used = f"logreg({solver})"

    metrics = {
        "encoder": "sbert" if not args.features else encoder_used,
        "model": model_name,
        "trainer": trainer_used,
        "train": evaluate(X_train, y_train, "Train"),
        "validation": evaluate(X_val, y_val, "Validation"),
        "test": evaluate(X_test, y_test, "Test")
    }

    if hasattr(clf, "n_iter_"):
        metrics["solver_iterations"] = (
            clf.n_iter_.tolist() if hasattr(clf.n_iter_, "tolist") else clf.n_iter_
        )

    args.output.parent.mkdir(parents=True, exist_ok=True)
    with args.output.open("w", encoding="utf-8") as f:
        json.dump(metrics, f, indent=2)
    print(f"Saved metrics to {args.output}")

if __name__ == "__main__":
    main()
